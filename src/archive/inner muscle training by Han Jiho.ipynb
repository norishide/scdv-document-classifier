{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.12.25 word remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import filterfalse\n",
    "\n",
    "\n",
    "def word_remover(words):\n",
    "    implies = dict.fromkeys(words, True).get # creating function str -> (True or None)\n",
    "    remove_words= lambda doc: list(filterfalse(implies, doc))\n",
    "    return remove_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.get(key, default=None, /)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.fromkeys(['qwe','asd','zxc'], True).get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwe': 3.5, 'asd': 3.5, 'zxc': 3.5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [T], a -> {T: a}\n",
    "# arguments type: iterable and value\n",
    "# return type: dict\n",
    "\n",
    "l1 = ['qwe','asd','zxc']\n",
    "v1 = 3.5\n",
    "dict.fromkeys(l1, v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwe': None, 'asd': None, 'zxc': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = ['qwe','asd','zxc']\n",
    "v1 = 3.5\n",
    "dict.fromkeys(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwe': True, 'asd': True, 'zxc': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.fromkeys(l1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'err')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how dict.get() works\n",
    "dict.fromkeys(l1, True).get('qwe'), dict.fromkeys(l1, True).get('hoge', 'err') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook doesn't show None\n",
    "dict.fromkeys(l1, True).get('hoge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# to show None...\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NoneType, True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not None means True\n",
    "type(None), not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, False, True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True, False, not True, not False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bool, bool)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(True), type(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None True\n"
     ]
    }
   ],
   "source": [
    "# creating function\n",
    "# arg type: str\n",
    "# return type: True or None\n",
    "implies = dict.fromkeys(l1, True).get \n",
    "print(implies('wer'), implies('qwe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wer', 'sdf', 'xcv']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter \n",
    "doc = ['qwe','zxc','wer','sdf','xcv']\n",
    "list(filterfalse(implies, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wer', 'sdf', 'xcv', 'ert', 'dfg', 'xcv']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter function\n",
    "# arg type: [str]\n",
    "# return type: [str]\n",
    "\n",
    "remove_words= lambda doc: list(filterfalse(implies, doc))\n",
    "doc = ['qwe','zxc','wer','sdf','xcv', 'ert','dfg','xcv','qwe','asd']\n",
    "remove_words(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.12.25 random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see white board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from operator import attrgetter, getitem, invert, itemgetter, methodcaller\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def to_categorical(y, n_classes):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # input_shape = y.shape\n",
    "    n_samples = y.shape[0]\n",
    "    categorical = np.full((n_samples, n_classes), 0, dtype=np.int8)\n",
    "    categorical[np.arange(n_samples), y] = 1\n",
    "    # output_shape = input_shape + (n_classes,)\n",
    "    # return np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "def best_splitter(max_features,\n",
    "                  class_weight,\n",
    "                  random_state=None,\n",
    "                  criterion=lambda proba_stride: 1 - np.einsum('ij,ij->i', proba_stride, proba_stride)):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    def node_split(X, y_encoded):\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        features = np.random.choice(n_features, max_features, replace=False)\n",
    "        \n",
    "        n_labels = np.einsum('ij->j', y_encoded) * class_weight\n",
    "        n_samples = np.einsum('i->', n_labels)\n",
    "        impurity = criterion((n_labels / n_samples)[np.newaxis, :])\n",
    "        \n",
    "        thresholds = [None] * max_features\n",
    "        max_info_gains = [None] * max_features\n",
    "\n",
    "        for i, x in enumerate(X.T[features]):\n",
    "\n",
    "            edges, freq = np.unique(x, return_counts=True)\n",
    "            \n",
    "            if edges.shape[0] < 2:\n",
    "                thresholds[i] = np.nan\n",
    "                max_info_gains[i] = 0.\n",
    "                \n",
    "            else:\n",
    "                rank = x.argsort()\n",
    "\n",
    "                n_samples_stride_left = freq[:-1].cumsum()\n",
    "\n",
    "                n_labels_stride_left = y_encoded[rank].cumsum(axis=0)[n_samples_stride_left - 1] * class_weight\n",
    "                n_labels_stride_right = n_labels[np.newaxis, :] - n_labels_stride_left\n",
    "\n",
    "                n_samples_stride_left = np.einsum('ij->i', n_labels_stride_left)\n",
    "                n_samples_stride_right = np.einsum('ij->i', n_labels_stride_right)\n",
    "\n",
    "                proba_stride_left = n_labels_stride_left / n_samples_stride_left[:, np.newaxis]\n",
    "                proba_stride_right = n_labels_stride_right / n_samples_stride_right[:, np.newaxis]\n",
    "\n",
    "                info_gains = criterion(proba_stride_right) * n_samples_stride_right\n",
    "                info_gains += criterion(proba_stride_left) * n_samples_stride_left\n",
    "                info_gains /= -n_samples\n",
    "                info_gains += impurity\n",
    "\n",
    "                idx = np.argmax(info_gains)\n",
    "                thresholds[i] = np.mean(edges[idx:idx + 2])\n",
    "                max_info_gains[i] = info_gains[idx]\n",
    "\n",
    "        idx = np.argmax(max_info_gains)\n",
    "        feature, threshold, info_gain = map(itemgetter(idx), (features, thresholds, max_info_gains))\n",
    "        \n",
    "        if np.isnan(threshold):\n",
    "            feature = -1\n",
    "            \n",
    "        return feature, threshold, info_gain\n",
    "\n",
    "    return node_split\n",
    "\n",
    "def build_depth_first_tree(X,\n",
    "                           y,\n",
    "                           node_split,\n",
    "                           max_depth=1,\n",
    "                           min_samples_split=2,\n",
    "                           min_samples_leaf=1,\n",
    "                           min_impurity_decrease=0.):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    UNDEFINED = -1\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    n_labels = np.bincount(y)\n",
    "    y_encoded = to_categorical(y, n_labels.shape[0]).astype(np.int64)\n",
    "    \n",
    "    stack = [(UNDEFINED, 0, 0, np.full(n_samples, True))]\n",
    "    push = stack.append\n",
    "    pop = stack.pop\n",
    "\n",
    "    tree = []\n",
    "    add_node = tree.append\n",
    "\n",
    "    while stack != []:\n",
    "        parent, depth, is_left, mask = pop()\n",
    "\n",
    "        node_id = len(tree)\n",
    "        n_samples = X[mask].shape[0]\n",
    "        feature = UNDEFINED\n",
    "        threshold = np.nan\n",
    "\n",
    "        is_leaf = (depth >= max_depth or\n",
    "                   n_samples < min_samples_split)\n",
    "\n",
    "        if not is_leaf:\n",
    "            feature, threshold, info_gain = node_split(X[mask], y_encoded[mask])\n",
    "            \n",
    "            if not np.isnan(threshold):\n",
    "                condition = X[mask, feature] >= threshold\n",
    "            \n",
    "                is_leaf = (info_gain <= min_impurity_decrease or\n",
    "                           min(condition.sum(), invert(condition).sum()) < min_samples_leaf)\n",
    "            \n",
    "            else:\n",
    "                is_leaf = 1\n",
    "\n",
    "            if is_leaf:\n",
    "                feature, threshold = UNDEFINED, np.nan\n",
    "\n",
    "        node = {\n",
    "            'node_id': node_id,\n",
    "            'parent': parent,\n",
    "            'depth': depth,\n",
    "            'is_leaf': int(is_leaf),\n",
    "            'is_left': is_left,\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'n_labels': np.einsum('ij->j', y_encoded[mask]),\n",
    "            'arity': 0\n",
    "        }\n",
    "        add_node(node)\n",
    "        \n",
    "        if parent != -1:\n",
    "            tree[parent]['arity'] += 1\n",
    "\n",
    "        if not is_leaf:\n",
    "            condition = X[:, feature] >= threshold\n",
    "            push((node_id, depth + 1, 0, mask & condition))\n",
    "            push((node_id, depth + 1, 1, mask & invert(condition)))\n",
    "            \n",
    "    return np.asarray(tree)\n",
    "\n",
    "get_tree_attribute = lambda tree, attr: np.asarray(list(map(itemgetter(attr), tree)))\n",
    "\n",
    "def search_subtree(tree, start):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    stop = start + 1\n",
    "    total = tree[start]['arity']\n",
    "    \n",
    "    while total > 0:\n",
    "        total += tree[stop]['arity'] - 1\n",
    "        stop += 1\n",
    "        \n",
    "    return slice(start, stop)\n",
    "\n",
    "def prune_tree(tree, start):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mask = np.full(tree.shape[0], False)\n",
    "    mask[search_subtree(tree, start)] = True\n",
    "    subtree = tree[mask]\n",
    "    n_labels = np.einsum('ij->j', get_tree_attribute(subtree, 'n_labels'))\n",
    "    \n",
    "    node = {\n",
    "        'node_id': start,\n",
    "        'parent': subtree[0]['parent'],\n",
    "        'depth': subtree[0]['depth'],\n",
    "        'is_leaf': 1,\n",
    "        'is_left': subtree[0]['is_left'],\n",
    "        'feature': -1,\n",
    "        'threshold': np.nan,\n",
    "        'n_labels': n_labels,\n",
    "        'arity': 0\n",
    "    }\n",
    "    \n",
    "    return np.insert(tree[invert(mask)], start, node)\n",
    "\n",
    "def compute_tree_impurity(tree,\n",
    "                          class_weight,\n",
    "                          criterion=lambda proba_stride: 1 - np.einsum('ij,ij->i', proba_stride, proba_stride)):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    leaf_nodes = tree[get_tree_attribute(tree, 'is_leaf') == 1]\n",
    "    n_labels_stride = get_tree_attribute(leaf_nodes, 'n_labels') * class_weight\n",
    "    n_samples_stride = np.einsum('ij->i', n_labels_stride)\n",
    "    proba_stride = n_labels_stride / n_samples_stride[:, np.newaxis]\n",
    "    impurity = np.einsum('i->', criterion(proba_stride) * n_samples_stride) / np.einsum('i->', n_samples_stride)\n",
    "\n",
    "    return leaf_nodes.shape[0], impurity\n",
    "\n",
    "def minimum_description_length(tree,\n",
    "                               class_weight,\n",
    "                               criterion=lambda proba_stride: 1 - np.einsum('ij,ij->i', proba_stride, proba_stride)):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_leaf_nodes_vs_tree_impurity  = []\n",
    "    for i in range(tree.shape[0]):\n",
    "        n_leaf_nodes_vs_tree_impurity.append(compute_tree_impurity(prune_tree(tree, i), class_weight, criterion))\n",
    "\n",
    "    return np.asarray(n_leaf_nodes_vs_tree_impurity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_data_sci)",
   "language": "python",
   "name": "conda_data_sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
