{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file '/home/ubuntu/.config/matplotlib/matplotlibrc' line #2.\n",
      "Duplicate key in file '/home/ubuntu/.config/matplotlib/matplotlibrc' line #3.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from functools import partial\n",
    "from itertools import count, filterfalse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import word2vec\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger\n",
    "def initialize_logger(dirpath='./output/pipeline'):\n",
    "    dirname = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    os.makedirs(os.path.join(dirpath, dirname), exist_ok=True)\n",
    "    \n",
    "    def logger(filename, obj):    \n",
    "        filepath = os.path.join(dirpath, dirname, filename)\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        if ext == '.npy':\n",
    "            np.save(filepath, obj)\n",
    "        elif ext == '.json':\n",
    "            json.dump(obj, open(filepath, 'w'), indent=4, ensure_ascii=False)\n",
    "        else :\n",
    "            raise Exception\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader\n",
    "def load_log(timestamp, filename, dirpath='./output/pipeline'):\n",
    "    filepath = os.path.join(dirpath, timestamp, filename)\n",
    "    \n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext == '.npy':\n",
    "        return np.load(filepath)\n",
    "    elif ext == '.json':\n",
    "        return json.load(open(filepath, 'r'))\n",
    "    else :\n",
    "        raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "# TODO: take into account preprocessing adn Japanese\n",
    "# code i wrote\n",
    "# def tokenize_document(doc: str) -> [str]:\n",
    "#     doc = doc.lower().replace('\\t','').replace('\\n','')\n",
    "#     doc = re.sub(re.compile(r'[!-\\/:-@[-`{-~]'), ' ', doc).split(' ')\n",
    "#     doc = [token for token in doc if len(token)>2]\n",
    "#     return [token for token in doc if token not in stop_words]\n",
    "\n",
    "stopwords = json.load(open('./Archive/stopwords.json'))\n",
    "\n",
    "def tokernizer(document, stopwords):\n",
    "    remove = dict.fromkeys(stopwords, True).get\n",
    "    raw_tokens = re.sub(r'\\W+', ' ', document.lower()).split(' ')\n",
    "    return list(filter(lambda token: len(token) > 1, filterfalse(remove, raw_tokens)))\n",
    "\n",
    "def tokenize_corpus(corpus: [str]) -> [[str]]: \n",
    "    corpus_tokenized = list(map(partial(tokernizer, stopwords=stopwords), corpus))\n",
    "    assert min(map(len, corpus_tokenized)) > 0\n",
    "    return corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word vectors by gensim\n",
    "def build_word_embedding_vectors(corpus_tokenized, word2vec_parameters):\n",
    "    model = word2vec.Word2Vec(corpus_tokenized, **word2vec_parameters)\n",
    "    return model.wv.vectors, model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# culster word vectors by GMM\n",
    "def culster_embedding_vectors(word_embeddings, gmm_parameters) -> np.ndarray:\n",
    "    X = word_embeddings\n",
    "    gm = GaussianMixture(**gmm_parameters)\n",
    "    gm.fit(X)\n",
    "    return gm.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF takes 20 min\n",
    "from collections import Counter\n",
    "from itertools import chain, repeat\n",
    "from operator import countOf\n",
    "\n",
    "\n",
    "def build_tfidf_selfmade(corpus, vocab):\n",
    "    countup = lambda doc: list(map(countOf, repeat(doc), vocab))\n",
    "    tf = np.array(list(map(countup, corpus)))\n",
    "    idf = np.log(len(corpus) / (tf > 0).sum(axis=0)) + 1\n",
    "    return tf, idf, tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def build_tfidf_sklearn(corpus_tokenized, vocab):\n",
    "    tfv = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', strip_accents='unicode', dtype=np.float64)\n",
    "    tfv.fit(list(map(' '.join, corpus_tokenized)))\n",
    "    feature_names = tfv.get_feature_names()\n",
    "    idf = tfv._tfidf.idf_\n",
    "    print(len(feature_names), len(idf), len(vocab))\n",
    "    return np.fromiter(map(dict(zip(feature_names, idf)).get, vocab), dtype='f8')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_topic_vector(word_embedding_vectors, word_cluster_probabilities, idf_vector):\n",
    "    wcv = np.einsum('ij,ik->ijk', word_embedding_vectors, word_cluster_probabilities).reshape(word_embedding_vectors.shape[0], -1)\n",
    "    return np.einsum('i,ij->ij', idf_vector, wcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document vectors\n",
    "def count(i=0):\n",
    "    while True:\n",
    "        yield i\n",
    "        i+=1\n",
    "\n",
    "def normalize_document_vectors(dv: np.ndarray) -> np.ndarray:\n",
    "    dv_sum = np.square(dv).sum(axis=1)\n",
    "    dv_l2norm = np.sqrt(dv_sum)\n",
    "    return np.einsum('ij,i->ij', dv, 1.0/dv_l2norm)\n",
    "\n",
    "def build_document_vectors(vocabulary, corpus_tokenized, word_topic_vectors):\n",
    "    vocabulary_idx_map = dict(zip(vocabulary, count()))\n",
    "    \n",
    "    document_vecotrs = []\n",
    "    for doc in corpus_tokenized:\n",
    "        doc_idx = list(filter(lambda idx: idx is not None, map(vocabulary_idx_map.get, doc)))\n",
    "        document_vecotrs.append(np.einsum('ij->j',word_topic_vectors[doc_idx]))\n",
    "    \n",
    "    return normalize_document_vectors(np.array(document_vecotrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCDV\n",
    "def make_sparse(document_vectors, param):\n",
    "    t = 0.5 * (np.abs(np.min(document_vectors, axis=1).mean()) + np.abs(np.max(document_vectors, axis=1).mean()))\n",
    "    sparsity_threshold = param * t\n",
    "    mask = np.abs(document_vectors) < sparsity_threshold\n",
    "    return np.where(mask, 0.0, document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "corpus = newsgroups_train['data']\n",
    "corpus_tokenized = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_parameters = {\n",
    "    'size':200,   # Word vector dimensionality\n",
    "    'min_count':20,    # Minimum word count\n",
    "    'workers':40,    # Number of threads to run in parallel\n",
    "    'window':10,    # Context window size\n",
    "    'sample':1e-3,    # Downsample setting for frequent words\n",
    "    'hs':0,\n",
    "    'sg':1,\n",
    "    'negative':10,\n",
    "    'iter':25,\n",
    "    'seed':1\n",
    "}\n",
    "\n",
    "word_embedding_vectors, vocabulary = build_word_embedding_vectors(corpus_tokenized, word2vec_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_parameters = {\n",
    "    'n_components':60,\n",
    "    'random_state':42,\n",
    "    'covariance_type':'tied',\n",
    "    'init_params':'kmeans',\n",
    "    'max_iter':50\n",
    "}\n",
    "\n",
    "word_cluster_probability_matrix = culster_embedding_vectors(word_embedding_vectors, gmm_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = initialize_logger()\n",
    "logger('word_cluster_probabilities.npy', word_cluster_probability_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster_probability_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vector = build_tfidf_sklearn(corpus_tokenized, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_vectors = build_word_topic_vector(word_embedding_vectors, word_cluster_probability_matrix, idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = build_document_vectors(vocabulary, corpus_tokenized, word_topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectorst_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger('word_topic_vectors.npy', word_topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = build_document_vectors(vocabulary, corpus_tokenized, word_topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger('document_vectors.npy', document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 0.04\n",
    "scdv = make_sparse(document_vectors, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = initialize_logger()\n",
    "logger('scdv.npy', scdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scdv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = load_log('20200126T082742Z','document_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "corpus = newsgroups_train['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scdv\n",
    "y = newsgroups_train['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_splitter(n_samples, n_folds, rng):\n",
    "    shuffled_fold_indices = rng.permutation(np.arange(n_samples) % n_folds)\n",
    "    for k in range(n_folds):\n",
    "        mask = shuffled_fold_indices == k\n",
    "        yield tuple(map(np.flatnonzero, (mask, ~mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, max_depth=None, max_features='sqrt',\n",
    "                             class_weight='balanced', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "n_samples = y.size\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for test_idx, train_idx in kfold_splitter(n_samples, n_fold, rng):\n",
    "    y_test = y[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    X_train = X[train_idx]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_train)\n",
    "    bas = balanced_accuracy_score(y_train, y_pred)\n",
    "    accuracy_train.append(bas)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    bas = balanced_accuracy_score(y_test, y_pred)\n",
    "    accuracy_test.append(bas)\n",
    "\n",
    "accuracy_train = np.array(accuracy_train)\n",
    "accuracy_test = np.array(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "corpus = newsgroups_train['data']\n",
    "y = newsgroups_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(list(map(' '.join, corpus_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "len(set(chain.from_iterable(corpus_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(chain.from_iterable(corpus_tokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_splitter(n_samples, n_folds, rng):\n",
    "    shuffled_fold_indices = rng.permutation(np.arange(n_samples) % n_folds)\n",
    "    for k in range(n_folds):\n",
    "        mask = shuffled_fold_indices == k\n",
    "        yield tuple(map(np.flatnonzero, (mask, ~mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, max_depth=None, max_features='sqrt',\n",
    "                             class_weight='balanced', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "n_samples = y.size\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for test_idx, train_idx in kfold_splitter(n_samples, n_fold, rng):\n",
    "    \n",
    "    y_test = y[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    X_train = X[train_idx]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_train)\n",
    "    bas = balanced_accuracy_score(y_train, y_pred)\n",
    "    accuracy_train.append(bas)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    bas = balanced_accuracy_score(y_test, y_pred)\n",
    "    accuracy_test.append(bas)\n",
    "\n",
    "accuracy_train = np.array(accuracy_train)\n",
    "accuracy_test = np.array(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.average(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file '/home/ubuntu/.config/matplotlib/matplotlibrc' line #2.\n",
      "Duplicate key in file '/home/ubuntu/.config/matplotlib/matplotlibrc' line #3.\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "import re\n",
    "# from mojimoji import han_to_zen, zen_to_han\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial, reduce\n",
    "from itertools import chain, count, filterfalse, repeat\n",
    "from operator import countOf, itemgetter, methodcaller\n",
    "\n",
    "compose = lambda *funcs: reduce(lambda f, g: lambda *args, **kwargs: f(g(*args, **kwargs)), funcs)\n",
    "flip = lambda f: lambda *args: f(*args[::-1])\n",
    "pipe = flip(compose)\n",
    "\n",
    "\n",
    "# str -> list\n",
    "split_text_into_lines = methodcaller('splitlines')\n",
    "# str -> str\n",
    "convert_text_to_lowercase = methodcaller('lower')\n",
    "# str -> str\n",
    "replace_whitespace_with_blank_char = partial(re.sub, \"\\s\", \" \")\n",
    "# str -> str\n",
    "remove_whitespace = partial(re.sub, \"\\s\", \"\")\n",
    "# str -> str\n",
    "strip_multiple_blank_chars_to_one = partial(re.sub, \" +\", \" \")\n",
    "# str -> str\n",
    "strip_consecutive_chars_to_one = lambda char='ー': partial(re.sub, '{}+'.format(char), '{}'.format(char))\n",
    "\n",
    "# str -> str\n",
    "def insert_text_between_blank_chars(text, patterns):\n",
    "    repls = map(\" {} \".format, patterns)\n",
    "    for pattern, repl in zip(patterns, repls):\n",
    "        text = text.replace(pattern, repl)\n",
    "    return text\n",
    "\n",
    "# str -> str\n",
    "def remove_whitespace_between_japanese_chars(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    chars = \"[、。〃々〆〇ぁ-んゔゝゞァ-ンヴヵヶ・ーヽヾ一-龥]\"\n",
    "    pattern = \"(?<={chars})\\s+(?={chars})\".format(chars=chars)\n",
    "    repl = \"\"\n",
    "    return re.sub(pattern, repl, text)\n",
    "\n",
    "# str -> list\n",
    "parse_tab_separated_text = re.compile(\"\\t\").split\n",
    "noisy_symbols = list('!\"#$%&\\'()*+,-./:;<=>?@[]^_`{|}¢£¥§¨¬°±´¶×÷‐―‖\"†‡‥…‰′″※℃Å←↑→↓⇒⇔∀∂∃∇∈∋−√∝∞∠∧∨∩∪∫∬∴∵∽≒≠≡≦≧≪≫⊂⊃⊆⊇⊥⌒─━│┃┌┏┐┓└┗┘┛├┝┠┣┤┥┨┫┬┯┰┳┴┷┸┻┼┿╂╋■□▲△▼▽◆◇○◎●◯★☆♀♂♪♭♯〈〉《》「」『』【】〒〓〔〕〜゛゜・＼｀￣（）。、”’｀？！')\n",
    "\n",
    "def maybe_lemmatize(mecab_node):\n",
    "    for surface, feature in map(re.compile('\\t').split, mecab_node.splitlines()[:-1]):\n",
    "        if len(feature.split(',')) == 7:\n",
    "            yield surface\n",
    "        else:\n",
    "            yield feature.split(',')[6]\n",
    "\n",
    "def remove_noisy_symbols(raw_tokens):\n",
    "    remove = dict.fromkeys(noisy_symbols, True).get\n",
    "    return filterfalse(remove, raw_tokens)\n",
    "\n",
    "preprocess = pipe(\n",
    "    str,\n",
    "#     han_to_zen,\n",
    "#     partial(zen_to_han, kana=False),\n",
    "    convert_text_to_lowercase,\n",
    "    replace_whitespace_with_blank_char,\n",
    "    remove_whitespace_between_japanese_chars,\n",
    "    strip_consecutive_chars_to_one(),\n",
    "    partial(insert_text_between_blank_chars, patterns=noisy_symbols),\n",
    "    strip_multiple_blank_chars_to_one,\n",
    "#     partial(re.sub, '\\d', '0'),\n",
    "    MeCab.Tagger().parse,\n",
    "    maybe_lemmatize,\n",
    "    remove_noisy_symbols,\n",
    "    list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ja(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [preprocess(line) for line in lines] \n",
    "        return list(chain.from_iterable(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "categories = ['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax',\n",
    "            'sports-watch', 'topic-news']\n",
    "\n",
    "data = pd.DataFrame([(c, tokenize_ja(path)) for c in categories for path in glob.glob(f'./text/{c}/*.txt')],\n",
    "                            columns=['category', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null\n",
    "data.loc[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized_ja = data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = initialize_logger()\n",
    "logger('corpus_tokenized_ja.json', corpus_tokenized_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized_ja = load_log('20200129T105243Z','corpus_tokenized_ja.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_tokenized_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview japanese corpus\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len_tokens_ja = list(map(len, corpus_tokenized_ja))\n",
    "plt.hist(len_tokens_ja, bins=40)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_tokenized_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(chain.from_iterable(corpus_tokenized_ja))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = corpus_tokenized_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_parameters = {\n",
    "    'size':200,   # Word vector dimensionality\n",
    "    'min_count':20,    # Minimum word count\n",
    "    'workers':40,    # Number of threads to run in parallel\n",
    "    'window':10,    # Context window size\n",
    "    'sample':1e-3,    # Downsample setting for frequent words\n",
    "    'hs':0,\n",
    "    'sg':1,\n",
    "    'negative':10,\n",
    "    'iter':25,\n",
    "    'seed':1\n",
    "}\n",
    "\n",
    "word_embedding_vectors, vocabulary = build_word_embedding_vectors(corpus_tokenized, word2vec_parameters)\n",
    "\n",
    "# logger = initialize_logger()\n",
    "logger('word_embedding_vectors.npy', word_embedding_vectors)\n",
    "logger('vocabulary.json', vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_vectors.shape, len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word_cluster_probability_matrix = load_log('20200129T105243Z', 'vocabulary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_parameters = {\n",
    "    'n_components':80,\n",
    "    'random_state':42,\n",
    "    'covariance_type':'tied',\n",
    "    'init_params':'kmeans',\n",
    "    'max_iter':50\n",
    "}\n",
    "\n",
    "word_cluster_probability_matrix = culster_embedding_vectors(word_embedding_vectors, gmm_parameters)\n",
    "\n",
    "logger('word_cluster_probabilities.npy', word_cluster_probability_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster_probability_matrix = load_log('20200127T154202Z', 'word_cluster_probabilities.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster_probability_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(word_cluster_probability_matrix.ravel(), bins=500, range=(0,1))\n",
    "plt.yscale('log')\n",
    "plt.xlim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf_vector = build_tfidf_sklearn(corpus_tokenized, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(idf_vector).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(vocabulary)[np.isnan(idf_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, idf, tfidf = build_tfidf_selfmade(corpus_tokenized, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(idf).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(idf, idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_vectors = build_word_topic_vector(word_embedding_vectors, word_cluster_probability_matrix, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = build_document_vectors(vocabulary, corpus_tokenized, word_topic_vectors)\n",
    "\n",
    "param = 0.04\n",
    "scdv = make_sparse(document_vectors, param)\n",
    "\n",
    "logger('scdv.npy', scdv)\n",
    "scdv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv = load_log('20200129T105243Z', 'scdv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(scdv.ravel(), bins=20)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dokujo-tsushin</td>\n",
       "      <td>./text/dokujo-tsushin/dokujo-tsushin-5934284.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dokujo-tsushin</td>\n",
       "      <td>./text/dokujo-tsushin/dokujo-tsushin-4848785.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dokujo-tsushin</td>\n",
       "      <td>./text/dokujo-tsushin/dokujo-tsushin-5247353.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dokujo-tsushin</td>\n",
       "      <td>./text/dokujo-tsushin/dokujo-tsushin-5653189.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dokujo-tsushin</td>\n",
       "      <td>./text/dokujo-tsushin/dokujo-tsushin-6218046.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>topic-news</td>\n",
       "      <td>./text/topic-news/topic-news-5914835.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>topic-news</td>\n",
       "      <td>./text/topic-news/topic-news-6403803.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>topic-news</td>\n",
       "      <td>./text/topic-news/topic-news-6703537.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>topic-news</td>\n",
       "      <td>./text/topic-news/topic-news-6723717.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>topic-news</td>\n",
       "      <td>./text/topic-news/topic-news-6681547.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7376 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            category                                              text\n",
       "0     dokujo-tsushin  ./text/dokujo-tsushin/dokujo-tsushin-5934284.txt\n",
       "1     dokujo-tsushin  ./text/dokujo-tsushin/dokujo-tsushin-4848785.txt\n",
       "2     dokujo-tsushin  ./text/dokujo-tsushin/dokujo-tsushin-5247353.txt\n",
       "3     dokujo-tsushin  ./text/dokujo-tsushin/dokujo-tsushin-5653189.txt\n",
       "4     dokujo-tsushin  ./text/dokujo-tsushin/dokujo-tsushin-6218046.txt\n",
       "...              ...                                               ...\n",
       "7371      topic-news          ./text/topic-news/topic-news-5914835.txt\n",
       "7372      topic-news          ./text/topic-news/topic-news-6403803.txt\n",
       "7373      topic-news          ./text/topic-news/topic-news-6703537.txt\n",
       "7374      topic-news          ./text/topic-news/topic-news-6723717.txt\n",
       "7375      topic-news          ./text/topic-news/topic-news-6681547.txt\n",
       "\n",
       "[7376 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = data['category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = dict(zip(np.unique(category),count(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(map(encoder.get, category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7376, 16000), (7376,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = scdv\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_splitter(n_samples, n_folds, rng):\n",
    "    shuffled_fold_indices = rng.permutation(np.arange(n_samples) % n_folds)\n",
    "    for k in range(n_folds):\n",
    "        mask = shuffled_fold_indices == k\n",
    "        yield tuple(map(np.flatnonzero, (mask, ~mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, max_depth=None, max_features='sqrt',\n",
    "                             class_weight='balanced', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "n_samples = y.size\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for test_idx, train_idx in kfold_splitter(n_samples, n_fold, rng):\n",
    "    y_test = y[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    X_train = X[train_idx]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_train)\n",
    "    bas = balanced_accuracy_score(y_train, y_pred)\n",
    "    accuracy_train.append(bas)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    bas = balanced_accuracy_score(y_test, y_pred)\n",
    "    accuracy_test.append(bas)\n",
    "\n",
    "accuracy_train = np.array(accuracy_train)\n",
    "accuracy_test = np.array(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf - ja - class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "categories = ['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax',\n",
    "            'sports-watch', 'topic-news']\n",
    "\n",
    "data = pd.DataFrame([(c, path) for c in categories for path in glob.glob(f'./text/{c}/*.txt')],\n",
    "                            columns=['category', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = data['category'].tolist()\n",
    "encoder = dict(zip(np.unique(category),count(1)))\n",
    "y = np.array(list(map(encoder.get, category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized_ja = load_log('20200129T105243Z', 'corpus_tokenized_ja.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = load_log('20200129T105243Z', 'vocabulary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11940"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, idf, tfidf = build_tfidf_selfmade(corpus_tokenized_ja, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7376, 11940)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7376, 11940), (7376,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_splitter(n_samples, n_folds, rng):\n",
    "    shuffled_fold_indices = rng.permutation(np.arange(n_samples) % n_folds)\n",
    "    for k in range(n_folds):\n",
    "        mask = shuffled_fold_indices == k\n",
    "        yield tuple(map(np.flatnonzero, (mask, ~mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, max_depth=None, max_features='sqrt',\n",
    "                             class_weight='balanced', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "n_samples = y.size\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for test_idx, train_idx in kfold_splitter(n_samples, n_fold, rng):\n",
    "    y_test = y[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    X_train = X[train_idx]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_train)\n",
    "    bas = balanced_accuracy_score(y_train, y_pred)\n",
    "    accuracy_train.append(bas)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    bas = balanced_accuracy_score(y_test, y_pred)\n",
    "    accuracy_test.append(bas)\n",
    "\n",
    "accuracy_train = np.array(accuracy_train)\n",
    "accuracy_test = np.array(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91243121, 0.91286318, 0.9024295 , 0.91307296, 0.91009114])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.910177598335179"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_1(x):\n",
    "    return x + 1\n",
    "def add_2(x):\n",
    "    return x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "reverse = itemgetter(slice(None, None, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def flip(f):\n",
    "    return lambda *args: f(*reverse(args))\n",
    "def compose(*fs):\n",
    "    return reduce(lambda f, g: lambda *args, **kwargs: f(g(*args, **kwargs)), fs)\n",
    "pipe = flip(compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add3 = pipe(add_1, add_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add3(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_3(3) # equivalent to `add_2(add_1(3))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemgetter(slice(None, None, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = list([1,3,5,7,9])\n",
    "arr[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_data_sci)",
   "language": "python",
   "name": "conda_data_sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
